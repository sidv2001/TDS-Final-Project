{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from typing import Dict, List, Set, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "val_df = pd.read_csv('data/val.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_playlists_by_Y_count(df, top_k):\n",
    "    \"\"\"\n",
    "    Remove playlists that have fewer than top_k 'Y' labels.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame containing the playlist data.\n",
    "    - top_k: Minimum number of 'Y' labels required.\n",
    "\n",
    "    Returns:\n",
    "    - filtered_df: DataFrame after removing playlists with fewer than top_k 'Y' labels.\n",
    "    \"\"\"\n",
    "    # Group by 'playlist_id' and count the number of 'Y' labels\n",
    "    Y_counts = df[df['XY'] == 'Y'].groupby('playlist_id').size().reset_index(name='Y_count')\n",
    "\n",
    "    # Filter playlists that have at least top_k 'Y' labels\n",
    "    valid_playlists = Y_counts[Y_counts['Y_count'] >= top_k]['playlist_id']\n",
    "\n",
    "    # Filter the original DataFrame to include only valid playlists\n",
    "    filtered_df = df[df['playlist_id'].isin(valid_playlists)].reset_index(drop=True)\n",
    "\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_Y_counts(df, top_k, set_name):\n",
    "    Y_counts = df[df['XY'] == 'Y'].groupby('playlist_id').size()\n",
    "    if Y_counts.min() >= top_k:\n",
    "        print(f\"All playlists in the {set_name} set have at least {top_k} 'Y' songs.\")\n",
    "    else:\n",
    "        print(f\"Some playlists in the {set_name} set have fewer than {top_k} 'Y' songs.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Assume 'merged_df' is prepared as before, containing only the necessary columns\n",
    "\n",
    "# Create a unique song ID using 'track_clean' and 'artist_clean'\n",
    "merged_df['song_id'] = merged_df['track_clean'] + ' - ' + merged_df['artist_clean']\n",
    "\n",
    "# Create a unique playlist ID using 'playlistname' and 'user_id'\n",
    "merged_df['playlist_id'] = merged_df['playlistname'] + ' - ' + merged_df['user_id']\n",
    "\n",
    "\n",
    "\n",
    "train_df['song_id'] = train_df['track_clean'] + ' - ' + train_df['artist_clean']\n",
    "\n",
    "# Create a unique playlist ID using 'playlistname' and 'user_id'\n",
    "train_df['playlist_id'] = train_df['playlistname'] + ' - ' + train_df['user_id']\n",
    "\n",
    "\n",
    "\n",
    "val_df['song_id'] = val_df['track_clean'] + ' - ' + val_df['artist_clean']\n",
    "\n",
    "# Create a unique playlist ID using 'playlistname' and 'user_id'\n",
    "val_df['playlist_id'] = val_df['playlistname'] + ' - ' + val_df['user_id']\n",
    "\n",
    "\n",
    "\n",
    "test_df['song_id'] = test_df['track_clean'] + ' - ' + test_df['artist_clean']\n",
    "\n",
    "# Create a unique playlist ID using 'playlistname' and 'user_id'\n",
    "test_df['playlist_id'] = test_df['playlistname'] + ' - ' + test_df['user_id']\n",
    "\n",
    "# Keep only necessary columns\n",
    "columns_to_keep = ['playlist_id', 'song_id', 'XY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_playlists = merged_df['playlist_id'].nunique()\n",
    "total_songs = merged_df['song_id'].nunique()\n",
    "print(f\"Total unique playlists: {total_playlists}\")\n",
    "print(f\"Total unique songs: {total_songs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of top_k\n",
    "top_k = 5  # Adjust as needed\n",
    "\n",
    "# Apply the function to each dataset\n",
    "train_df_filtered = filter_playlists_by_Y_count(train_df, top_k)\n",
    "val_df_filtered = filter_playlists_by_Y_count(val_df, top_k)\n",
    "test_df_filtered = filter_playlists_by_Y_count(test_df, top_k)\n",
    "\n",
    "# Verify that all playlists have at least top_k 'Y' songs\n",
    "verify_Y_counts(train_df_filtered, top_k, 'Train')\n",
    "verify_Y_counts(val_df_filtered, top_k, 'Validation')\n",
    "verify_Y_counts(test_df_filtered, top_k, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mapping(train_data):\n",
    "  unique_playlists = train_data['playlist_id'].unique()\n",
    "  unique_songs = merged_df['song_id'].unique()\n",
    "\n",
    "  # Create mappings\n",
    "  playlist_id_to_index = {playlist_id: idx for idx, playlist_id in enumerate(unique_playlists)}\n",
    "  song_id_to_index = {song_id: idx for idx, song_id in enumerate(unique_songs)}\n",
    "\n",
    "  # For reverse mapping if needed\n",
    "  index_to_playlist_id = {idx: playlist_id for playlist_id, idx in playlist_id_to_index.items()}\n",
    "  index_to_song_id = {idx: song_id for song_id, idx in song_id_to_index.items()}\n",
    "  return playlist_id_to_index, song_id_to_index, index_to_playlist_id, index_to_song_id\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def generate_interaction_matrix(train_df, playlist_id_to_index, song_id_to_index):\n",
    "  # Generate mappings\n",
    "  playlist_id_to_index, song_id_to_index, index_to_playlist_id, index_to_song_id = generate_mapping(train_df)\n",
    "  # Filter to 'X' entries\n",
    "  interaction_df = train_df\n",
    "\n",
    "  # Map playlist and song IDs to indices\n",
    "  interaction_df['playlist_index'] = interaction_df['playlist_id'].map(playlist_id_to_index)\n",
    "  interaction_df['song_index'] = interaction_df['song_id'].map(song_id_to_index)\n",
    "\n",
    "  # Prepare data for the sparse matrix\n",
    "  row_indices = interaction_df['playlist_index'].values\n",
    "  col_indices = interaction_df['song_index'].values\n",
    "  data_values = np.ones(len(interaction_df))  # Since we're marking presence, use 1s\n",
    "\n",
    "  # Create the sparse interaction matrix\n",
    "  num_playlists = len(playlist_id_to_index)\n",
    "  num_songs = len(song_id_to_index)\n",
    "\n",
    "  interaction_matrix = coo_matrix(\n",
    "      (data_values, (row_indices, col_indices)),\n",
    "      shape=(num_playlists, num_songs)\n",
    "  ).tocsr()\n",
    "  return interaction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert interaction matrix to a dense format if necessary (not recommended for large datasets)\n",
    "# Since the interaction matrix is large, we'll work with the sparse format\n",
    "from sklearn.preprocessing import normalize\n",
    "# Use the interaction_matrix directly for SVD\n",
    "# Note: For large sparse matrices, consider using implicit matrix factorization methods\n",
    "\n",
    "# Define the number of latent factors\n",
    "def generate_song_vectors(interaction_matrix, k=50):\n",
    "  # Compute the mean (not centering since it's implicit feedback data)\n",
    "  # For implicit feedback, we often skip mean centering\n",
    "\n",
    "  # Apply SVD\n",
    "  # Since scipy's svds works with csr_matrix, we can use it directly\n",
    "  U, sigma, Vt = svds(interaction_matrix, k=k)\n",
    "\n",
    "  # Convert sigma to a diagonal matrix\n",
    "  sigma = np.diag(sigma)\n",
    "\n",
    "  # Compute the predicted ratings\n",
    "  R_pred = U.dot(sigma).dot(Vt)\n",
    "\n",
    "  # Convert to csr_matrix for efficient row access\n",
    "  R_pred_matrix = csr_matrix(R_pred)\n",
    "\n",
    "\n",
    "  song_factors = Vt.T\n",
    "  return song_factors, normalize(song_factors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming you have:\n",
    "# - song_factors: numpy array of shape (num_songs, k)\n",
    "# - index_to_song_id: mapping from song index to song ID\n",
    "# - song_id_to_index: mapping from song ID to song index\n",
    "\n",
    "# Normalize song factors for cosine similarity\n",
    "# song_factors_normalized = normalize(song_factors)\n",
    "\n",
    "def recommend_songs_for_playlist(\n",
    "    X_song_ids,\n",
    "    song_factors,\n",
    "    song_factors_normalized,\n",
    "    song_id_to_index,\n",
    "    index_to_song_id,\n",
    "    top_k=25\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate song recommendations for a playlist given a list of songs in 'X'.\n",
    "\n",
    "    Parameters:\n",
    "    - X_song_ids: List of song IDs in the playlist's 'X' set.\n",
    "    - song_factors: Numpy array of song latent factors (num_songs x k).\n",
    "    - song_factors_normalized: Normalized song factors for similarity computation.\n",
    "    - song_id_to_index: Mapping from song IDs to indices.\n",
    "    - index_to_song_id: Mapping from indices to song IDs.\n",
    "    - top_k: Number of recommendations to generate.\n",
    "\n",
    "    Returns:\n",
    "    - recommended_songs: List of recommended song IDs.\n",
    "    \"\"\"\n",
    "    # Map song IDs to indices\n",
    "    X_song_indices = [song_id_to_index[song_id] for song_id in X_song_ids if song_id in song_id_to_index]\n",
    "\n",
    "    if not X_song_indices:\n",
    "        print(\"No valid songs in the playlist.\")\n",
    "        return []\n",
    "\n",
    "    # Get song factors for songs in the playlist\n",
    "    X_song_factors = song_factors[X_song_indices]\n",
    "\n",
    "    # Compute the playlist's latent factor (e.g., mean of song factors)\n",
    "    playlist_vector = np.mean(X_song_factors, axis=0).reshape(1, -1)\n",
    "\n",
    "    # Normalize the playlist vector\n",
    "    playlist_vector_normalized = normalize(playlist_vector)\n",
    "\n",
    "    # Compute cosine similarity between the playlist vector and all song factors\n",
    "    similarity_scores = cosine_similarity(playlist_vector_normalized, song_factors_normalized).flatten()\n",
    "\n",
    "    # Exclude songs already in the playlist\n",
    "    similarity_scores[X_song_indices] = -np.inf  # Set their scores to negative infinity\n",
    "\n",
    "    # Get top K song indices\n",
    "    top_song_indices = np.argpartition(similarity_scores, -top_k)[-top_k:]\n",
    "    top_song_indices = top_song_indices[np.argsort(-similarity_scores[top_song_indices])]\n",
    "\n",
    "    # Map indices back to song IDs\n",
    "    recommended_songs = [index_to_song_id[idx] for idx in top_song_indices]\n",
    "\n",
    "    return recommended_songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_for_sets(df, song_factors, song_factors_normalized, song_id_to_index, index_to_song_id, set_name, top_k=25):\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    playlists = df['playlist_id'].unique()\n",
    "\n",
    "    for i in tqdm(range(len(playlists))):\n",
    "        playlist_id = playlists[i]\n",
    "        playlist_df = df[df['playlist_id'] == playlist_id]\n",
    "        X_songs = playlist_df[playlist_df['XY'] == 'X']['song_id'].tolist()\n",
    "        Y_songs = playlist_df[playlist_df['XY'] == 'Y']['song_id'].tolist()\n",
    "\n",
    "        if not X_songs or not Y_songs:\n",
    "            continue\n",
    "\n",
    "        # Generate recommendations\n",
    "        recommended_songs = recommend_songs_for_playlist(\n",
    "            X_song_ids=X_songs,\n",
    "            song_factors=song_factors,\n",
    "            song_factors_normalized=song_factors_normalized,\n",
    "            song_id_to_index=song_id_to_index,\n",
    "            index_to_song_id=index_to_song_id,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Compute metrics\n",
    "        num_Y_in_rec = len(set(Y_songs) & set(recommended_songs))\n",
    "        recall = num_Y_in_rec / len(Y_songs)\n",
    "        precision = num_Y_in_rec / top_k  # Since top_k=25\n",
    "\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "\n",
    "    print(f\"{set_name} Set - Average Recall: {avg_recall:.4f}, Average Precision: {avg_precision:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REPETITIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = [0.01, 0.05, 0.10, 0.25, 0.50]\n",
    "train_results = [[]] * NUM_REPETITIONS\n",
    "validation_results = [[]] * NUM_REPETITIONS\n",
    "test_results = [[]] * NUM_REPETITIONS\n",
    "# Create the subsets\n",
    "for i in range(NUM_REPETITIONS):\n",
    "    for percentage in percentages:\n",
    "        subset_size = int(len(train_df_filtered) * percentage)\n",
    "        subset = train_df_filtered.sample(n=subset_size, random_state=42)\n",
    "        playlist_id_to_index, song_id_to_index, index_to_playlist_id, index_to_song_id = generate_mapping(subset)\n",
    "        interaction_matrix = generate_interaction_matrix(subset, playlist_id_to_index, song_id_to_index)\n",
    "        song_factors, song_factors_normalized = generate_song_vectors(interaction_matrix)\n",
    "        print(f\"Results on {percentage}% of the training data used\")\n",
    "        train_res = compute_metrics_for_sets(train_df, song_factors, song_factors_normalized, song_id_to_index, index_to_song_id, 'Train', top_k=5)\n",
    "        val_res = compute_metrics_for_sets(val_df, song_factors, song_factors_normalized, song_id_to_index, index_to_song_id, 'Validation', top_k=5)\n",
    "        test_res = compute_metrics_for_sets(test_df, song_factors, song_factors_normalized, song_id_to_index, index_to_song_id, 'Test', top_k=5)\n",
    "        train_results[i].append(train_res)\n",
    "        validation_results[i].append(val_res)\n",
    "        test_results[i].append(test_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Features Based Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the feature columns are from index 5 to 20 (adjust if necessary)\n",
    "feature_columns = merged_df.columns[5:20]  # Python indexing is end-exclusive\n",
    "\n",
    "# Extract song features\n",
    "song_features_df = merged_df[['song_id'] + list(feature_columns)].drop_duplicates(\"song_id\").reset_index(drop=True)\n",
    "\n",
    "# Handle missing values if any\n",
    "song_features_df[feature_columns] = song_features_df[feature_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(song_features_df.shape, len(merged_df['song_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "song_features_scaled = scaler.fit_transform(song_features_df[feature_columns])\n",
    "\n",
    "# Convert to a DataFrame\n",
    "song_features_scaled_df = pd.DataFrame(song_features_scaled, columns=feature_columns)\n",
    "song_features_scaled_df['song_id'] = song_features_df['song_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map song IDs to indices\n",
    "song_features_scaled_df['song_index'] = song_features_scaled_df['song_id'].map(song_id_to_index)\n",
    "\n",
    "# Ensure the order matches the song indices\n",
    "song_features_scaled_df = song_features_scaled_df.sort_values('song_index').reset_index(drop=True)\n",
    "\n",
    "# Extract the feature matrix\n",
    "song_feature_matrix = song_features_scaled_df[feature_columns].values\n",
    "\n",
    "# Normalize the feature matrix for cosine similarity\n",
    "song_feature_matrix_normalized = normalize(song_feature_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_with_features(\n",
    "    X_song_ids,\n",
    "    song_feature_matrix,\n",
    "    song_feature_matrix_normalized,\n",
    "    song_id_to_index,\n",
    "    index_to_song_id,\n",
    "    top_k=25\n",
    "):\n",
    "    \"\"\"\n",
    "    Recommend songs based on pre-existing song features.\n",
    "\n",
    "    Parameters:\n",
    "    - X_song_ids: List of song IDs in the playlist's 'X' set.\n",
    "    - song_feature_matrix: Numpy array of song features.\n",
    "    - song_feature_matrix_normalized: Normalized song features for similarity computation.\n",
    "    - song_id_to_index: Mapping from song IDs to indices.\n",
    "    - index_to_song_id: Mapping from indices to song IDs.\n",
    "    - top_k: Number of recommendations to generate.\n",
    "\n",
    "    Returns:\n",
    "    - recommended_songs: List of recommended song IDs.\n",
    "    \"\"\"\n",
    "    # Map song IDs to indices\n",
    "    X_song_indices = [song_id_to_index[song_id] for song_id in X_song_ids if song_id in song_id_to_index]\n",
    "\n",
    "    if not X_song_indices:\n",
    "        print(\"No valid songs in the playlist.\")\n",
    "        return []\n",
    "\n",
    "    # Get features for songs in the playlist\n",
    "    X_song_features = song_feature_matrix[X_song_indices]\n",
    "\n",
    "    # Compute the playlist's feature vector (mean of song features)\n",
    "    playlist_vector = np.mean(X_song_features, axis=0).reshape(1, -1)\n",
    "\n",
    "    # Normalize the playlist vector\n",
    "    playlist_vector_normalized = normalize(playlist_vector)\n",
    "\n",
    "    # Compute cosine similarity between the playlist vector and all song features\n",
    "    similarity_scores = cosine_similarity(playlist_vector_normalized, song_feature_matrix_normalized).flatten()\n",
    "\n",
    "    # Exclude songs already in the playlist\n",
    "    similarity_scores[X_song_indices] = -np.inf  # Set their scores to negative infinity\n",
    "\n",
    "    # Get top K song indices\n",
    "    top_song_indices = np.argpartition(similarity_scores, -top_k)[-top_k:]\n",
    "    top_song_indices = top_song_indices[np.argsort(-similarity_scores[top_song_indices])]\n",
    "\n",
    "    # Map indices back to song IDs\n",
    "    recommended_songs = [index_to_song_id[idx] for idx in top_song_indices]\n",
    "\n",
    "    return recommended_songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_features(df, song_feature_matrix, song_feature_matrix_normalized, song_id_to_index, index_to_song_id, set_name, top_k=25):\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    playlists = df['playlist_id'].unique()\n",
    "\n",
    "    for i in tqdm(range(len(playlists))):\n",
    "        playlist_id = playlists[i]\n",
    "        playlist_df = df[df['playlist_id'] == playlist_id]\n",
    "        X_songs = playlist_df[playlist_df['XY'] == 'X']['song_id'].tolist()\n",
    "        Y_songs = playlist_df[playlist_df['XY'] == 'Y']['song_id'].tolist()\n",
    "\n",
    "        if not X_songs or not Y_songs:\n",
    "            continue\n",
    "\n",
    "        # Generate recommendations\n",
    "        recommended_songs = recommend_songs_with_features(\n",
    "            X_song_ids=X_songs,\n",
    "            song_feature_matrix=song_feature_matrix,\n",
    "            song_feature_matrix_normalized=song_feature_matrix_normalized,\n",
    "            song_id_to_index=song_id_to_index,\n",
    "            index_to_song_id=index_to_song_id,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Compute metrics\n",
    "        num_Y_in_rec = len(set(Y_songs) & set(recommended_songs))\n",
    "        recall = num_Y_in_rec / len(Y_songs)\n",
    "        precision = num_Y_in_rec / top_k  # Since top_k=25\n",
    "\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "\n",
    "    print(f\"{set_name} Set - Average Recall: {avg_recall:.4f}, Average Precision: {avg_precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compute_metrics_with_features(train_df, song_feature_matrix, song_feature_matrix_normalized, song_id_to_index, index_to_song_id, 'Train', top_k=5)\n",
    "compute_metrics_with_features(val_df, song_feature_matrix, song_feature_matrix_normalized, song_id_to_index, index_to_song_id, 'Validation', top_k=5)\n",
    "compute_metrics_with_features(test_df, song_feature_matrix, song_feature_matrix_normalized, song_id_to_index, index_to_song_id, 'Test', top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_precision_recall(method1_name, method2_name,\n",
    "                          train_precision_method1, test_precision_method1,\n",
    "                          train_recall_method1, test_recall_method1,\n",
    "                          train_precision_method2, test_precision_method2,\n",
    "                          train_recall_method2, test_recall_method2):\n",
    "    \"\"\"\n",
    "    Plots bar graphs for precision and recall of two methods for train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - method1_name, method2_name: Names of the methods.\n",
    "    - train_precision_method1, test_precision_method1: Precision for method1 (train, test).\n",
    "    - train_recall_method1, test_recall_method1: Recall for method1 (train, test).\n",
    "    - train_precision_method2, test_precision_method2: Precision for method2 (train, test).\n",
    "    - train_recall_method2, test_recall_method2: Recall for method2 (train, test).\n",
    "    \"\"\"\n",
    "    # Data preparation\n",
    "    methods = [method1_name, method2_name]\n",
    "    precision_train = [train_precision_method1, train_precision_method2]\n",
    "    precision_test = [test_precision_method1, test_precision_method2]\n",
    "    recall_train = [train_recall_method1, train_recall_method2]\n",
    "    recall_test = [test_recall_method1, test_recall_method2]\n",
    "\n",
    "    x = np.arange(len(methods))  # Label locations\n",
    "    width = 0.35  # Bar width\n",
    "\n",
    "    # Plotting Precision\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bars1 = ax.bar(x - width/2, precision_train, width, label='Train Precision')\n",
    "    bars2 = ax.bar(x + width/2, precision_test, width, label='Test Precision')\n",
    "\n",
    "    # Labels and formatting\n",
    "    ax.set_xlabel('Methods', fontsize=12)\n",
    "    ax.set_ylabel('Precision', fontsize=12)\n",
    "    ax.set_title('Precision Comparison', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(methods, fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.bar_label(bars1, fmt='%.4f', padding=3)\n",
    "    ax.bar_label(bars2, fmt='%.4f', padding=3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting Recall\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bars3 = ax.bar(x - width/2, recall_train, width, label='Train Recall')\n",
    "    bars4 = ax.bar(x + width/2, recall_test, width, label='Test Recall')\n",
    "\n",
    "    # Labels and formatting\n",
    "    ax.set_xlabel('Methods', fontsize=12)\n",
    "    ax.set_ylabel('Recall', fontsize=12)\n",
    "    ax.set_title('Recall Comparison', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(methods, fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.bar_label(bars3, fmt='%.4f', padding=3)\n",
    "    ax.bar_label(bars4, fmt='%.4f', padding=3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_precision_recall(\n",
    "    method1_name=\"Item Based Collaborative Filtering\", method2_name=\"Song Audio Features based Reccomender\",\n",
    "    train_precision_method1=0.0298, test_precision_method1=0.0204,\n",
    "    train_recall_method1=0.0257, test_recall_method1=0.0193,\n",
    "    train_precision_method2=0.0026, test_precision_method2=0.003,\n",
    "    train_recall_method2=0.0026, test_recall_method2=0.0029\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_recommender_performance(knn_precision, knn_recall, percentages, precision_values, recall_values):\n",
    "    \"\"\"\n",
    "    Plots precision and recall values for a KNN-based recommender and item-based collaborative filtering models.\n",
    "    \n",
    "    Parameters:\n",
    "        knn_precision (float): Precision value of the KNN-based model.\n",
    "        knn_recall (float): Recall value of the KNN-based model.\n",
    "        percentages (list of float): List of percentages (as decimals) representing training data proportions.\n",
    "        precision_values (list of float): Precision values for item-based CF models.\n",
    "        recall_values (list of float): Recall values for item-based CF models.\n",
    "    \"\"\"\n",
    "    # Create labels for the x-axis\n",
    "    labels = ['Audio Features (KNN)'] + [f'Item-based CF\\n{int(p * 100)}%' for p in percentages]\n",
    "    \n",
    "    # Combine KNN values with the list of item-based CF values\n",
    "    all_precisions = [knn_precision] + precision_values\n",
    "    all_recalls = [knn_recall] + recall_values\n",
    "    \n",
    "    # Create x positions for the bars\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    # Set bar width\n",
    "    bar_width = 0.5\n",
    "    \n",
    "    # Create the figure and axes\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    # Plot precision bar graph\n",
    "    bars_prec = axs[0].bar(x, all_precisions, color=['orange'] + ['blue'] * len(percentages), width=bar_width)\n",
    "    axs[0].set_title('Model Precision')\n",
    "    axs[0].set_ylabel('Precision')\n",
    "    axs[0].set_xticks(x)\n",
    "    axs[0].set_xticklabels(labels, rotation=45, ha='right')\n",
    "    axs[0].set_ylim(0, 0.025)\n",
    "    axs[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values above precision bars\n",
    "    for bar in bars_prec:\n",
    "        axs[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.0005, \n",
    "                    f'{bar.get_height():.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot recall bar graph\n",
    "    bars_recall = axs[1].bar(x, all_recalls, color=['orange'] + ['blue'] * len(percentages), width=bar_width)\n",
    "    axs[1].set_title('Model Recall')\n",
    "    axs[1].set_ylabel('Recall')\n",
    "    axs[1].set_xticks(x)\n",
    "    axs[1].set_xticklabels(labels, rotation=45, ha='right')\n",
    "    axs[1].set_ylim(0, 0.025)\n",
    "    axs[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values above recall bars\n",
    "    for bar in bars_recall:\n",
    "        axs[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.0005, \n",
    "                    f'{bar.get_height():.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Adjust layout and show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# KNN model precision and recall\n",
    "knn_precision = 0.003\n",
    "knn_recall = 0.0029\n",
    "\n",
    "# Percentages of training data used\n",
    "percentages = [0.01, 0.05, 0.10, 0.25, 0.50, 1.0]\n",
    "\n",
    "# Precision and recall values for item-based CF models\n",
    "precision_values = [0.0012, 0.0024, 0.0032,  0.0081, 0.0155, 0.0204]\n",
    "recall_values = [0.0007, 0.0015, 0.0023,  0.0068, 0.0140, 0.0193]\n",
    "\n",
    "# Generate the plot\n",
    "plot_recommender_performance(knn_precision, knn_recall, percentages, precision_values, recall_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
